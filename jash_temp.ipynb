{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"project.env\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai as oai\n",
    "\n",
    "import langchain_openai as lcai\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, PromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "\n",
    "from utils import mLangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "llmchat = lcai.AzureChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=\"NUHAI-GPT4\",\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    "    model_name=\"gpt-4\",\n",
    ")\n",
    "\n",
    "embeddings = lcai.AzureOpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    deployment=\"TEST-Embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llmchat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "\n",
    "qa_info_prompt = \"\"\"\n",
    "            Your role is to act like a CUSTOMER seeking support. \\\n",
    "            The user is a support representative. \\\n",
    "            Respond to the question as if you were the customer. \\\n",
    "            If the user is asking for a specific detail, respond with a believable answer.\n",
    "        \"\"\"\n",
    "qa_info = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_info_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "rag_chain_info = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=contextualize_q_chain\n",
    "        )\n",
    "        | qa_info\n",
    "        | llmchat\n",
    ")\n",
    "\n",
    "qa_uncivil_prompt = \"\"\"\n",
    "            Given a history of messages, where the AI is a customer and the user is a representative, rephrase the response to the representative's message.\\\n",
    "            \n",
    "            Rephrase the customer response to sound UNCIVIL. \\\n",
    "            Do NOT reply to the question ONLY rephrase. \\\n",
    "            \n",
    "            This is what UNCIVIL customers do:\\\n",
    "            - Address others in an unprofessional, disrespectful way-for example, talking down, using degrading remarks or tone of voice.\\\n",
    "            - Pay little or no attention to others’ opinions.\\\n",
    "            - Use intimidating or threatening verbal communication—yelling, repeated emotional outbursts, threats, berating or harsh tone of voice, repeatedly interrupting.\\\n",
    "            - Blaming others for things out of their control.\\\n",
    "            - Accusing others of incompetence or dismissing their expertise.\\\n",
    "        \"\"\"\n",
    "qa_uncivil = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_uncivil_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", '''\n",
    "            Representative asked: {question}\n",
    "            Customer responded: {input}\n",
    "        '''),\n",
    "    ]\n",
    ")\n",
    "chain_uncivil = (RunnablePassthrough.assign(\n",
    "                    context=contextualize_q_chain\n",
    "                )\n",
    "                 | qa_uncivil\n",
    "                 | llmchat\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response 1: I'm really sorry to hear that your room was not in the condition you expected when you checked in. This is certainly not the standard we aim to uphold. Could you please provide me with your room number and the name the reservation is under? We'll have our housekeeping team address this immediately, and I'll also look into what might have happened with our cleaning process. Additionally, we'd like to offer you a complimentary service to make up for this inconvenience. Would you prefer a room upgrade, a free meal at our restaurant, or perhaps something else? Please let us know how we can make your stay more comfortable.\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"I just checked into my room and found that it hasn't been cleaned properly. There are towels from the previous guest and the trash hasn't been emptied. \n",
    "This isn't what I expected from a well-rated hotel.\"\"\"\n",
    "customer_complaint = [HumanMessage(content=content)]\n",
    "chat_history = []\n",
    "chat_history.append(content)\n",
    "ai_response = rag_chain_info.invoke({'chat_history': chat_history, 'question': customer_complaint})\n",
    "chat_history.append(ai_response.content)\n",
    "print(\"AI Response 1:\", ai_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response 2: I completely understand your frustration and apologize for the inconvenience. Let me arrange a different room for you right away. Could you please provide me with your current room number so I can expedite this process? We'll ensure the new room is thoroughly inspected and meets our high standards before you move in. I'll also make sure this is handled immediately.\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"How soon can I expect someone to clean it? You know what, I want a different room. Completely disgusted.\"\"\"\n",
    "customer_followup = [HumanMessage(content=content)]\n",
    "chat_history.append(content)\n",
    "ai_response_2 = rag_chain_info.invoke({'chat_history': chat_history, 'question': customer_followup})\n",
    "chat_history.append(ai_response_2.content)\n",
    "print(\"AI Response 2:\", ai_response_2.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
